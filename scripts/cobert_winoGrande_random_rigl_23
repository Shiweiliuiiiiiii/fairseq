#!/bin/bash
#SBATCH --job-name=cobert_winogrande_random_rigl_seed23
#SBATCH -p gpu
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=1
#SBATCH --gpus=1
#SBATCH -t 1-24:00:00
#SBATCH --cpus-per-task=18
#SBATCH -o cobert_winogrande_random_rigl_seed23.out

source /home/sliu/miniconda3/etc/profile.d/conda.sh
source activate transformer


for seed in 2 3
do

TOTAL_NUM_UPDATES=23750 # Total number of training steps.
WARMUP_UPDATES=2375     # Linearly increase LR over this many steps.
LR=1e-05                # Peak LR for polynomial LR scheduler.
MAX_SENTENCES=32        # Batch size per GPU.
SEED=$seed                  # Random seed.
ROBERTA_PATH=~/project_space/pruning_fails/QA/pretrained_models/roberta.large/model.pt
DATA_DIR=/home/sliu/project_space/pruning_fails/QA/robert/winogrande/winogrande_1.1/
# we use the --user-dir option to load the task and criterion
# from the examples/roberta/wsc directory:
FAIRSEQ_PATH=/home/sliu/Projects/fairseq
FAIRSEQ_USER_DIR=${FAIRSEQ_PATH}/examples/roberta/wsc

for sparsity in 0.2 0.36 0.488 0.590 0.672 0.738 0.791 0.8325 0.866 0.893
do
save_dir=/home/sliu/project_space/pruning_fails/QA/robert/winogrande/WSC/random_rigl_seed$SEED/$sparsity
CUDA_VISIBLE_DEVICES=0 python train.py $DATA_DIR \
  --restore-file $ROBERTA_PATH \
  --save-dir $save_dir \
  --reset-optimizer --reset-dataloader --reset-meters \
  --no-epoch-checkpoints --no-last-checkpoints --no-save-optimizer-state \
  --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \
  --valid-subset val \
  --fp16 --ddp-backend legacy_ddp \
  --user-dir $FAIRSEQ_USER_DIR \
  --task winogrande --criterion winogrande \
  --wsc-margin-alpha 5.0 --wsc-margin-beta 0.4 \
  --arch roberta_large --bpe gpt2 --max-positions 512 \
  --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \
  --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 \
  --lr-scheduler polynomial_decay --lr $LR \
  --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_NUM_UPDATES \
  --batch-size $MAX_SENTENCES \
  --max-update $TOTAL_NUM_UPDATES \
  --log-format simple --log-interval 100 --sparse-init random --sparse-mode DST --sparsity $sparsity --sparse \
  --prune magnitude --prune-rate 0.5 --growth gradient --update-frequency 500 --redistribution none --seed $SEED
done
done


#python examples/roberta/commonsense_qa/evaluate.py
