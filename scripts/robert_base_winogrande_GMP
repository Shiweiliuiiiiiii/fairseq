TOTAL_NUM_UPDATES=23750 # Total number of training steps.
WARMUP_UPDATES=2375     # Linearly increase LR over this many steps.
LR=1e-05                # Peak LR for polynomial LR scheduler.
MAX_SENTENCES=32        # Batch size per GPU.
SEED=1                  # Random seed.
ROBERTA_PATH=/home/shiwei/Projects/fairseq/examples/roberta/pretrained_models/roberta.base/model.pt
DATA_DIR=/home/shiwei/Projects/fairseq/data/winogrande_1.1/
# we use the --user-dir option to load the task and criterion
# from the examples/roberta/wsc directory:

FAIRSEQ_USER_DIR=/home/shiwei/Projects/fairseq/examples/roberta/wsc

for sparsity in 0.2 0.36 0.488 0.590 0.672 0.738 0.791 0.8325 0.866 0.893
do
save_dir=$FAIRSEQ_USER_DIR/GMP/$sparsity
CUDA_VISIBLE_DEVICES=2 python train.py $DATA_DIR \
  --restore-file $ROBERTA_PATH \
  --save-dir $save_dir \
  --reset-optimizer --reset-dataloader --reset-meters \
  --no-epoch-checkpoints --no-last-checkpoints --no-save-optimizer-state \
  --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \
  --valid-subset val \
  --ddp-backend legacy_ddp \
  --user-dir $FAIRSEQ_USER_DIR \
  --task winogrande --criterion winogrande \
  --wsc-margin-alpha 5.0 --wsc-margin-beta 0.4 \
  --arch roberta_base --bpe gpt2 --max-positions 512 \
  --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \
  --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 \
  --lr-scheduler polynomial_decay --lr $LR \
  --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_NUM_UPDATES \
  --batch-size $MAX_SENTENCES \
  --max-update $TOTAL_NUM_UPDATES \
  --log-format simple --log-interval 100 --sparse-init dense --sparse-mode GMP --sparsity $sparsity --sparse \
  --prune magnitude --prune-rate 0.5 --growth gradient --update-frequency 500 --redistribution none

rm -rf &save_dir/
done